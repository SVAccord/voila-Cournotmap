{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Courno map builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''Seek for conflicts in structures eqg passed\n",
    "'''\n",
    "# take care about param types\n",
    "# нужно получать структуру на вход по которой уже разбирать на подгруппы\n",
    "#\n",
    "\n",
    "import re\n",
    "#import functions_is as fis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string # for int2base\n",
    "\n",
    "#from itertools import product\n",
    "from itertools import combinations\n",
    "#from itertools import permutations\n",
    "import itertools\n",
    "#import timeit\n",
    "#import datetime\n",
    "import copy\n",
    "# Dependencies (webapp)\n",
    "#from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "#import io\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def listToString(list1):\n",
    "    return (''.join(map(str, list1)))\n",
    "def diversity_num(vec, dim):\n",
    "    # return num of different digits without nans\n",
    "    dvst = 0\n",
    "    for i in range(0, dim):\n",
    "        for j in range(0, len(vec)):\n",
    "            if vec[j] == i:\n",
    "                dvst += 1\n",
    "                break\n",
    "    return dvst\n",
    "def list_getsome(vec, positions):\n",
    "    list_n = []\n",
    "    for y in positions:\n",
    "        list_n.append(vec[y])\n",
    "    #print(\"lnf\", list_n)\n",
    "    return(list_n)\n",
    "def diversity_vec(vec, dim):\n",
    "    dvst = []\n",
    "    for i in range(0, dim):\n",
    "        for j in range(0, len(vec)):\n",
    "            if vec[j] == i:\n",
    "                dvst.append(i)\n",
    "                break\n",
    "    return dvst\n",
    "def checkinpattern_str(txt, pattern):\n",
    "    # get strings\n",
    "    # if work w list from begining will be faster\n",
    "    ltxt = txt.split()\n",
    "    stxt = set(ltxt)\n",
    "    return stxt.issubset(set(pattern))\n",
    "\n",
    "def checkinpattern_list(ltxt, lpattern):\n",
    "    # get lists\n",
    "    stxt = set(ltxt)\n",
    "    return stxt.issubset(set(lpattern))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return positions of extern () , begin from start\n",
    "def openshell(f_text_v, start):\n",
    "    '''return what shell contais'''\n",
    "    mark = start\n",
    "    Term1_r_brct_ind, Term1_l_brct_ind = 0, 0\n",
    "    Term1_r_brct_cnt, Term1_l_brct_cnt = 0, 0\n",
    "    while mark <= len(f_text_v): \n",
    "        if( (f_text_v[mark] == '(') and (Term1_l_brct_cnt == 0) ): \n",
    "            Term1_l_brct_ind = mark # to save pos of f l_bracket\n",
    "        if(f_text_v[mark] == '('): Term1_l_brct_cnt = Term1_l_brct_cnt + 1\n",
    "        if(f_text_v[mark] == ')'):\n",
    "            if(Term1_r_brct_cnt >= Term1_l_brct_cnt ): return(-1) # like ...)*()\n",
    "            Term1_r_brct_cnt = Term1_r_brct_cnt + 1\n",
    "            Term1_r_brct_ind = mark\n",
    "        if( (Term1_l_brct_cnt != 0) and (Term1_r_brct_cnt == Term1_l_brct_cnt) ): break\n",
    "        mark = mark + 1\n",
    "    if(Term1_l_brct_cnt != Term1_r_brct_cnt): return(-1)\n",
    "    else:\n",
    "        return(f_text_v[(Term1_l_brct_ind+1):Term1_r_brct_ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def structdisassemble(stct):\n",
    "    los = []\n",
    "    # assume braket format input, like (a(bc)). Don't see how make it simple for l10, l11 and so on\n",
    "    for i in range(len(stct)):\n",
    "        if stct[i] == '(': \n",
    "            t = [s for s in list(openshell(stct, i))  if s.isalpha()]\n",
    "            los.append(t)\n",
    "    return los\n",
    "\n",
    "# don't think it is need to sort\n",
    "def l2sort(l2):\n",
    "    for i in range(0,len(l2)):\n",
    "        l2[i].sort()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the structure defined on set of examples can be realised \n",
    "# with incopmlete techniqe\n",
    "# блоки\n",
    "# 1. на основании структуры составление плана обрабоки групп\n",
    "# 2. применение функции проверки/склейки к подгруппам по схеме\n",
    "# адреса + функция дают разбиение на группы. На выход разбиение\n",
    "# как это зациклить?\n",
    "# ввожу размерность для адресов и для функции\n",
    "#def eqg_og(data_ini, structure):\n",
    "def eqg_groupcheck(data_ini, grouptoinvestigate):\n",
    "    #функция проверки\n",
    "    #составление карты Карно на основе таблицы и функции(разбиения)\n",
    "    # data_ini - dataframe\n",
    "    # grouptoinvestigate - names of columns to select\n",
    "    #print(data_ini)\n",
    "    #print(data_ini.columns)\n",
    "    #alphabet = ['l1', 'l2', 'l3', 'l4', 'l5']\n",
    "    alphabet = data_ini.columns\n",
    "    alphabet = list(alphabet)\n",
    "    alphabet.pop(len(alphabet)-1)\n",
    "\n",
    "#     grouptoinvestigate_p = []\n",
    "#     for i in range(0,len(grouptoinvestigate)):\n",
    "#         grouptoinvestigate_p.append(grouptoinvestigate[i] - 1)\n",
    "    #print(grouptoinvestigate)\n",
    "    \n",
    "    lnum = data_ini.shape[1]-1\n",
    "    #print(\"lnum = \", lnum)\n",
    "    #print(data_ini.columns[grouptoinvestigate])\n",
    "        \n",
    "    # w with data in pandas style\n",
    "    #data = np.array(data_ini.iloc[:,grouptoinvestigate_p])\n",
    "    fornames = copy.deepcopy(grouptoinvestigate)\n",
    "    #fornames = grouptoinvestigate\n",
    "    fornames.append('IR')\n",
    "    #print(fornames)\n",
    "    #print(grouptoinvestigate)\n",
    "    data = data_ini[fornames]\n",
    "    data_names = data.columns\n",
    "    data = np.array(data_ini[grouptoinvestigate])\n",
    "    #print(\"data \\n\", data)\n",
    "    #reminder_slave = list(set([i for i in range(data_ini.shape[1]-1)])-set(grouptoinvestigate_p))\n",
    "    reminder_slave = list(set(alphabet)-set(grouptoinvestigate))\n",
    "    #print(reminder_slave)\n",
    "    data_slave = np.array(data_ini[reminder_slave])\n",
    "    #print(\"2 \\n\", data_slave)\n",
    "    dataout = data\n",
    "    # let's input numpy. And outpput too\n",
    "    #data = data_ini\n",
    "#     for i in range(1,3):\n",
    "#         print(data[i, [i,i+1])\n",
    "\n",
    "    # w with data in numpy style\n",
    "    #dataout = data[:,grouptoinvestigate_p]\n",
    "    \n",
    "    # эта сортиовка нужна для вывода результатов в return.  \n",
    "    dataout = np.unique(dataout, axis=0)\n",
    "    \n",
    "    # нужно сигнализировать если были противоречивые строки\n",
    "    \n",
    "    #dataout = dataout[np.lexsort((dataout[:,0], dataout[:,1], dataout[:,2], dataout[:,3]))]\n",
    "    dataout = dataout[np.lexsort([dataout[:,i] for i in range(dataout.shape[1])])]\n",
    "   \n",
    "    \n",
    "    #from operator import itemgetter\n",
    "    #data1 = sorted(data1,key=itemgetter(0))\n",
    "    #print(\"data1: \\n\" ,dataout)\n",
    "    dim = 1+data.max()-data.min()\n",
    "    dim_slave = 1+data_slave.max()-data_slave.min()\n",
    "    #print('max', data.max())\n",
    "    #print(\"dim = \", (data.max()-data.min())+1)\n",
    "    #print(\"dim_slave = \", (data_slave.max()-data_slave.min())+1)\n",
    "    dim_f = 1+data_ini['IR'].max() - data_ini['IR'].min()\n",
    "    #print('dim_f', dim_f)\n",
    "    #if dim_f > dim and dim_f > dim_slave:\n",
    "    #    print(\"\\n OUT OF SCALE \\n\")\n",
    "    #print(\"K: \" ,data[:,lnum-1])\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    # all posible values of group numbers\n",
    "    lenumeration = [i for i in range(0,lnum)] \n",
    "\n",
    "    len_combinations = 0\n",
    "    for i in range(2,lnum):\n",
    "        lcombinations = list(combinations(range(1, lnum+1), i))\n",
    "        len_combinations += len(lcombinations)\n",
    "    numofgroup = len(grouptoinvestigate)\n",
    "    #print(numofgroup)\n",
    "    # form table K\n",
    "    #print(\"group of leaves investigated: \", grouptoinvestigate)#lcombinations_p[numberofvalue])\n",
    "    #print(\"\\n\")\n",
    "    tbl_k = np.empty((dim**numofgroup,dim_slave**(lnum-numofgroup)))\n",
    "    tbl_k.fill(np.nan)\n",
    "    colfilled = []\n",
    "    rowfilled = []\n",
    "    seen = set()\n",
    "    seen_row = set()\n",
    "\n",
    "    #enumeration_l_col = np.array(c1)\n",
    "    #enumeration_l_row = np.array(c2)\n",
    "    # fill table K from data string by sting (x)\n",
    "    for x in range(0,len(data)):\n",
    "        #print(data[x,lcombinations[j]])\n",
    "        # get coordinate in nondecimal representation of string\n",
    "        #print(data[x,lcombinations[numberofvalue]])\n",
    "        # get coordinate in nondecimal representation of column\n",
    "        \n",
    "        #reminder = list(set(lenumeration)-set(grouptoinvestigate_p))#lcombinations[numberofvalue])) \n",
    "        #print(data[x,reminder])\n",
    "        # need to place F value in Ktable to selected coordinates\n",
    "        c1 = \"\".join([str(xx) for xx in data[x,:]])#lcombinations[numberofvalue]]])\n",
    "        #print(c1)\n",
    "        c2 = \"\".join([str(xx) for xx in data_slave[x,:]])\n",
    "        #print(c2)\n",
    "        c1_t = int(c1[::-1], base = dim)\n",
    "        c2_t = int(c2[::-1], base = dim_slave)\n",
    "        #print(\"d\",c1_t)\n",
    "        #print(\"d\",c2_t)\n",
    "        tbl_k[c1_t, c2_t] = data_ini.iat[x,lnum]\n",
    "        # попутно можно записать все столбцы в которых есть значения, соотв. другие м.б. удал.\n",
    "        if c2_t not in seen:\n",
    "            seen.add(c2_t)\n",
    "            colfilled.append(c2_t) \n",
    "        if c1_t not in seen_row:\n",
    "            seen_row.add(c1_t)\n",
    "            rowfilled.append(c1_t) \n",
    "    #print(colfilled)\n",
    "\n",
    "    #print(rowfilled)\n",
    "    #print(tbl_k)\n",
    "    colfilled.sort()\n",
    "    #print(colfilled)\n",
    "\n",
    "    tbl_k_s = tbl_k[:,colfilled]\n",
    "    # del nan rows\n",
    "    # use rowfilled\n",
    "    rows_nan = np.all(np.isnan(tbl_k_s), axis = (1))\n",
    "    tbl_k_s = tbl_k_s[~rows_nan,:]\n",
    "    #print(tbl_k_s)\n",
    "\n",
    "    # отсортируй и конвертируй в нужную шкалу  colfilled\n",
    "    # +1 - convert number in range from 1, for table output\n",
    "    #colfilled = [i+1 for i in colfilled]\n",
    "    #colfilled.sort()\n",
    "\n",
    "    #print(colfilled)\n",
    "    #rowfilled = [i+1 for i in rowfilled]\n",
    "    rowfilled.sort()\n",
    "\n",
    "    colfilled = [np.base_repr(int(i), base=dim_slave) for i in colfilled]\n",
    "    colfilled.insert(0, 0)\n",
    "    colfilled = [\"{0:0>{1}}\".format(i, data_slave.shape[1])[::-1] for i in colfilled]\n",
    "    rowfilled = [np.base_repr(int(i), base=dim) for i in rowfilled]\n",
    "    rowfilled = [\"{0:0>{1}}\".format(i, lnum-data_slave.shape[1])[::-1] for i in rowfilled]\n",
    "    colfilled[0] = ''\n",
    "    # print(colfilled)\n",
    "    # print(rowfilled)\n",
    "    temp_k = np.hstack((np.array(rowfilled).reshape(len(tbl_k_s),1), tbl_k_s))\n",
    "    temp_k = np.vstack((np.array(colfilled).reshape(1,temp_k.shape[1]), temp_k))\n",
    "\n",
    "    # create numbers of str|col \n",
    "    numbers = [int(i) for i in range(1,len(tbl_k_s)+1)]\n",
    "    numbers.insert(0, \"\")\n",
    "    numbers_col = [int(i) for i in range(0,tbl_k_s.shape[1]+1)]\n",
    "    numbers_col.insert(0, \"\")\n",
    "    numbers_col[1] = ''\n",
    "    temp_k = np.hstack((np.array(numbers).reshape(len(temp_k),1), temp_k))\n",
    "    temp_k = np.vstack((np.array(numbers_col).reshape(1,temp_k.shape[1]), temp_k))\n",
    "    #print(tbl_k_s)\n",
    "    #print(temp_k)\n",
    "#     for i in range(0, temp_k.shape[0]):\n",
    "#         for j in range(0, temp_k.shape[1]):\n",
    "#              with data_output: print(\"{0:5}\".format(temp_k[i][j]), end = ' ')\n",
    "#     with data_output:print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    with data_output: display(pd.DataFrame(temp_k))\n",
    "            \n",
    "            \n",
    "            \n",
    "    #print(temp_k)\n",
    "\n",
    "    # for now it is necessary to analyze whether the given alphabet is enough\n",
    "    list_ind = [[i for i in range(tbl_k_s.shape[0])]]\n",
    "    #print(list_ind)\n",
    "    inigroup = 1\n",
    "\n",
    "    for r in range(tbl_k_s.shape[1]):\n",
    "        # to evade path dependance in table analysis - take r from end to begining\n",
    "\n",
    "        # select a column, from back because extend used further\n",
    "        # checklist - digits in curr col\n",
    "        #if len(set(tbl_k_s[:,r])) > 1:\n",
    "        if diversity_num(tbl_k_s[:,r], dim_f) > 1:\n",
    "            # if there is conflict of type 1 = more than 1 different digit in col\n",
    "            #checklist = tbl_k_s[:,r]\n",
    "            checklist = copy.deepcopy(tbl_k_s[:,r])\n",
    "            checklist_pos = 0\n",
    "            for z in range(len(list_ind)-1,-1,-1): \n",
    "                # select a range in list_ind\n",
    "                # rev list - это связано с тем что del удаляет с конца\n",
    "                # в этой версии это не нужно\n",
    "                ind_t = []\n",
    "                flag_del_inigroup = 0\n",
    "                nan_counted = 0\n",
    "                for w in range(dim_f):\n",
    "                    # search for every simbol in alphabet in column                      \n",
    "                    range_in_range = np.where(tbl_k_s[list_ind[z],r] == w)[0].tolist()\n",
    "                    #if r > 0 and len(range_in_range) and not nan_counted:\n",
    "                    if not inigroup and len(range_in_range) and not nan_counted:\n",
    "                        # inigroup for don't count nans when inigroup activated\n",
    "                        plus_na = np.argwhere(np.isnan(tbl_k_s[list_ind[z],r]))\n",
    "                        plus_na = list(itertools.chain(*plus_na))\n",
    "                        range_in_range = list(itertools.chain(*[range_in_range, plus_na]))\n",
    "                        nan_counted = 1\n",
    "                    if len(range_in_range):\n",
    "                        range_in_range = list_getsome(list_ind[z], range_in_range)\n",
    "                        checklist[range_in_range] = np.nan\n",
    "                        ind_t.append(range_in_range)        \n",
    "                        flag_del_inigroup = 1\n",
    "\n",
    "                if flag_del_inigroup:\n",
    "                    del list_ind[z]\n",
    "                    flag_del_inigroup = 0\n",
    "                    inigroup = 0\n",
    "                # sort list for further includes and dubles checking\n",
    "                for c in range(len(ind_t)):\n",
    "                    ind_t[c].sort()\n",
    "                list_ind.extend(ind_t)\n",
    "            # seek for opportunity to expand eqg by new digits\n",
    "            # checklist_pos - digits in checklist\n",
    "            if ~np.isnan(checklist).all():\n",
    "                # test if there are not nans\n",
    "                # каждую позицию из разнообразия нужно приклеить без противоречия\n",
    "                # берем доступные числа из групп столбца и расширяем группы по ним\n",
    "                checklist_pos = np.argwhere(~np.isnan(checklist))#[0].tolist()\n",
    "                checklist_pos = list(itertools.chain(*checklist_pos))                \n",
    "                # need to sort list_ind for groups with digits go first\n",
    "                zzt = 0\n",
    "                cnt_zzt = 0\n",
    "                while cnt_zzt < len(list_ind)-1:\n",
    "                    #if not len(set(tbl_k_s[list_ind[zzt],r])):\n",
    "                    if not diversity_num(tbl_k_s[list_ind[zzt],r], dim_f):\n",
    "                        #print(tbl_k_s[list_ind[zzt],r])\n",
    "                        #print(len(set(tbl_k_s[list_ind[zzt],r])))\n",
    "                        list_ind.append(list_ind[zzt])\n",
    "                        del list_ind[zzt]\n",
    "                    else: zzt += 1\n",
    "                    cnt_zzt += 1\n",
    "                for zz in range(len(list_ind)):\n",
    "                    ind_tt = list_ind[zz]\n",
    "                    cnt_j = 0 \n",
    "                    for j in checklist[checklist_pos]:\n",
    "                        j = int(j)\n",
    "                        ldiv = diversity_vec(tbl_k_s[ind_tt,r], dim_f)\n",
    "                        # проверка ГЭ на наличие символа отличного от того для которого мы ищем группу                           \n",
    "                        # сформировать список без j и искать вхождение из ldiv в него\n",
    "                        list_without_j = [i for i in range(dim_f)]\n",
    "                        #print(\"dim\", dim)\n",
    "                        #print(\"j\", j)\n",
    "                        #print(list_without_j)\n",
    "                        del list_without_j[np.nonzero(np.array(list_without_j) == j)[0][0]]\n",
    "                        #print(list_without_j)\n",
    "                        if any(item in ldiv for item in list_without_j):\n",
    "                            cnt_j += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            list_ind[zz].append(checklist_pos[cnt_j])\n",
    "                            checklist[checklist_pos[cnt_j]] = np.nan\n",
    "                            checklist_pos = np.argwhere(~np.isnan(checklist))#[0].tolist()\n",
    "                            checklist_pos = list(itertools.chain(*checklist_pos))\n",
    "            #else # если в ГЭ нет номера из checklist, то добваить группу\n",
    "            # проверить остались ли свободные числа in checklist\n",
    "            if ~np.isnan(checklist).all():\n",
    "                # для каждого отдельного номинала своя группа\n",
    "                ldiv2 = diversity_vec(checklist, dim_f)\n",
    "                for i in ldiv2:\n",
    "                    range_in_range2 = np.where(checklist == i)[0].tolist()\n",
    "                    list_ind.append(range_in_range2)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    list_ind_output = copy.deepcopy(list_ind)\n",
    "    for i in range(len(list_ind_output)):\n",
    "        for j in range(len(list_ind_output[i])):\n",
    "            list_ind_output[i][j] += 1\n",
    "    #print(\"final partition: \", list_ind_output)   \n",
    "    #print(\"groups found = \", len(list_ind_output))\n",
    "    #del list_ind_output\n",
    "\n",
    "    #postanalisys: check if result may be merged for each pair of groups\n",
    "    q = 0\n",
    "    while q <= len(list_ind)-2:\n",
    "        flag_onse = 1\n",
    "        u = q + 1\n",
    "        while u <= len(list_ind)-1:\n",
    "            tbl_merged = np.vstack((tbl_k_s[list_ind[q],:], tbl_k_s[list_ind[u],:]))\n",
    "            #print(tbl_merged)\n",
    "            for p in range(tbl_merged.shape[1]):\n",
    "                if len(set(tbl_merged[~np.isnan(tbl_merged[:,p]),p])) > 1:\n",
    "                    break           \n",
    "                if p == tbl_merged.shape[1]-1:\n",
    "                    # was no break till ther end, so found 2 groups that can be merged\n",
    "                    if flag_onse:\n",
    "                        #print(\"in result of postanalysis found opportunity for merge\")\n",
    "                        flag_onse = 0\n",
    "                    #print(list_ind[q], list_ind[u])\n",
    "                    #t = list(itertools.chain(*[list_ind[q], list_ind[u]]))\n",
    "                    list_ind.append(list(itertools.chain(*[list_ind[q], list_ind[u]])))\n",
    "                    del list_ind[q]\n",
    "                    del list_ind[u-1]\n",
    "                    #list_ind.append(t)\n",
    "                    list_ind_output = copy.deepcopy(list_ind)\n",
    "                    for i in range(len(list_ind_output)):\n",
    "                        for j in range(len(list_ind_output[i])):\n",
    "                            list_ind_output[i][j] += 1\n",
    "                    #print(\"final partition: \", list_ind_output)   \n",
    "                    #print(\"groups found = \", len(list_ind_output))\n",
    "                    #print(\"\\n\")\n",
    "            u += 1\n",
    "        q += 1\n",
    "    #print(rowfilled)\n",
    "    l_eqg = [0 for i in range(len(rowfilled))]\n",
    "    cnt = 0\n",
    "    for i in list_ind:\n",
    "        for j in i:\n",
    "            l_eqg[j] = cnt\n",
    "        cnt += 1\n",
    "    #print(l_eqg)\n",
    "    #return pd.DataFrame({'A':data, 'B':l_eqg})\n",
    "    #temp_k = np.hstack((np.array(rowfilled).reshape(len(tbl_k_s),1), tbl_k_s))\n",
    "    dataout = np.hstack((dataout, np.array(l_eqg).reshape(len(dataout),1)))\n",
    "    #data_names = list(data_names)\n",
    "    #data_names = data_names.append('IS')\n",
    "    #print(data_names)\n",
    "    return pd.DataFrame(dataout, columns = data_names)\n",
    "    #return dataout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnsselected = widgets.Text(\n",
    "#     value='ab',\n",
    "#     placeholder='Type something',\n",
    "#     description='Enter col splitting:',\n",
    "#     disabled=False\n",
    "# )\n",
    "# widgets.HBox([columnsselected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preloaded: \n",
      "    l1  l2  l3  l4  l5  IS\n",
      "0   2   1   1   2   0   2\n",
      "1   1   0   0   1   0   1\n",
      "2   1   1   2   0   2   2\n",
      "3   2   1   2   2   1   2\n",
      "4   1   1   0   1   1   2\n"
     ]
    }
   ],
   "source": [
    "#out.clear_output()\n",
    "data_ini = pd.read_csv('210929_id_arctic_l5_data.csv', sep=';')\n",
    "print(\"Data preloaded: \\n\", data_ini.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload your data and chouse group for splitting, from the data column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49613aba0eb34a7c8d341195d761e05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2760db700a4d27acd54949d33c2c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57dae81a0e44ad8869873d4477c2cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f7a1897a7f40a78cc47c9ffcd05290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed6be0d522440cab6333fa8025b4711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize outputs\n",
    "\n",
    "upload_btn_output = widgets.Output(clear_output=True)\n",
    "display(upload_btn_output)\n",
    "\n",
    "split_output = widgets.Output(clear_output=True)\n",
    "display(split_output)\n",
    "\n",
    "data_process_output = widgets.Output(clear_output=True)\n",
    "display(data_process_output)\n",
    "\n",
    "compute_btn_output = widgets.Output(clear_output=True)\n",
    "display(compute_btn_output)\n",
    "\n",
    "#Print out filtered\n",
    "data_output = widgets.Output(clear_output=True)\n",
    "display(data_output)\n",
    "\n",
    "\n",
    "#Event handler for upload button\n",
    "# можно загрузить свою таблицу из файла, вызов загрузки по кнопке просто ее перезапишет\n",
    "\n",
    "def upload_btn_eventhandler(obj):\n",
    "    global data_ini\n",
    "    data_process_output.clear_output(wait=True)\n",
    "    for file_name in upload_file.value:\n",
    "        extension = file_name.split('.')[1]\n",
    "        content = upload_file.value[file_name]['content']\n",
    "        if(extension == 'xlsx' or extension == 'xls'):\n",
    "            data_ini = pd.read_excel(BytesIO(content))           \n",
    "        elif(extension == 'csv'):\n",
    "            data_ini = pd.read_csv(BytesIO(content), sep = ';')        \n",
    "        else:\n",
    "            print(\"File not accepted\")\n",
    "        #with data_process_output:\n",
    "        #    print(f\"Processed a total of {len(data_ini)} transactions.\")\n",
    "        with data_process_output:\n",
    "            print(data_ini)\n",
    "    return data_ini\n",
    "                \n",
    "def compute_btn_eventhandler(obj):\n",
    "    global data_ini\n",
    "    data_output.clear_output(wait=True)\n",
    "    tablenames = list(string.ascii_lowercase[0:(data_ini.shape[1]-1)])\n",
    "    tablenames.append('IR')\n",
    "    #print(tablenames)\n",
    "    data_ini.columns = tablenames\n",
    "    #los = structdisassemble('((ab)cde)') \n",
    "    alphabet = data_ini.columns\n",
    "    alphabet = list(alphabet)\n",
    "    alphabet.pop(len(alphabet)-1)\n",
    "    \n",
    "    rmr = list(set(list(string.ascii_lowercase[0:(data_ini.shape[1]-1)]))-set(columnsselected.value))\n",
    "    \n",
    "    los = structdisassemble('(('+columnsselected.value+')'+str(rmr)+')') \n",
    "    \n",
    "    l2sort(los)\n",
    "    \n",
    "    ldf = []\n",
    "    ldf.append(data_ini)\n",
    "    #dict_losldf\n",
    "    dflosldf = pd.DataFrame(\n",
    "        {\n",
    "            \"group\": np.array(len(los), dtype=\"str\"),\n",
    "            \"dfadr\": np.empty(len(los), dtype=\"int32\"),\n",
    "            \"crash\": np.array(len(los), dtype=\"str\"),\n",
    "        }\n",
    "    )\n",
    "    cnt = 0\n",
    "    for i in range(len(los)):\n",
    "        dflosldf.iat[cnt,0] = ''.join(map(str, los[i]))\n",
    "        dflosldf.iat[cnt,1] = 0\n",
    "        dflosldf.iat[cnt,2] = \"\"\n",
    "        #print(\"dfnumofisg[i] \", dfnumofisg.iloc[cnt,:])\n",
    "        cnt += 1\n",
    "    #dflosldf\n",
    "    for i in range(1,len(los)):\n",
    "        foundindex = i - 1 \n",
    "        if i == 1:\n",
    "            #print('data used: \\n', ldf[i-1])\n",
    "            #with data_output:\n",
    "            #    display(ldf[i-1])\n",
    "            #print('group used: \\n', los[i])\n",
    "            #with data_process_output: display('group used: \\n', los[i])         \n",
    "            ldf.append(eqg_groupcheck(ldf[i-1], los[i]))\n",
    "            dflosldf.loc[dflosldf['group'] == ''.join(map(str, los[i])),'dfadr'] = len(ldf)-1\n",
    "        else:\n",
    "            # seek for dfadr to w with\n",
    "            #print('['+str(''.join(map(str, los[i])))+']')\n",
    "            structlikeseries = pd.Series(los[i])\n",
    "            getrowslikesets = list(dflosldf['group'].apply(set))\n",
    "            rowscontainstructurebool = [structlikeseries.isin(x).all() for x in getrowslikesets]\n",
    "            rowscontainstructure = dflosldf['group'][rowscontainstructurebool]\n",
    "            structureindexes = list(rowscontainstructure.index) # def if empty list\n",
    "            structureindexes.pop() # remove last (self)\n",
    "            foundindex = structureindexes.pop() # return second from end   \n",
    "            #print('data used: \\n', ldf[foundindex])\n",
    "            #with data_output:\n",
    "            #    display(ldf[foundindex])\n",
    "            #with data_output:\n",
    "            #    display('group used: \\n', los[i])\n",
    "            #print('group used: \\n', los[i])\n",
    "            ldf.append(eqg_groupcheck(ldf[foundindex], los[i]))\n",
    "            dflosldf.loc[dflosldf['group'] == ''.join(map(str, los[i])),'dfadr'] = len(ldf)-1\n",
    "        #print(dflosldf)\n",
    "        # anilize F dimension and break if it > scale\n",
    "\n",
    "        tcols = list(ldf[foundindex].columns)\n",
    "        tcols.pop()\n",
    "\n",
    "        tdf = pd.DataFrame(data=ldf[len(ldf)-1][\"IR\"])\n",
    "        tdf.rename(columns = {'IR':'grp'}, inplace = True)\n",
    "        tdf.index += 1\n",
    "        #tdf = pd.DataFrame(index = pd.Index(range(1,len(ldf[len(ldf)-1][\"IR\"]))), data = ldf[len(ldf)-1][\"IR\"])\n",
    "        #tdf = pd.DataFrame(data = ldf[len(ldf)-1][\"IR\"])\n",
    "        \n",
    "#         ttdf = {'index1':[i for i in range(1,len(ldf[len(ldf)-1][\"IR\"])+1)], \"data\":ldf[len(ldf)-1][\"IR\"]}\n",
    "#         tdf = pd.DataFrame(data = ttdf)\n",
    "#         tdf.set_index('index1')\n",
    "        \n",
    "        #tdf.set_index([pd.Index(range(1,len(tdf)+1))])\n",
    "        #tdf.reindex([range(1,len(tdf)+1)])\n",
    "        #tdf.reindex([i for i in range(1,len(tdf)+1)])\n",
    "            #display(\"IR: \\n\", ldf[len(ldf)-1][\"IR\"])\n",
    "        with data_output: display(tdf)\n",
    "        dim_adr =  max(ldf[len(ldf)-1].iloc[0:len(ldf[len(ldf)-1])-1, 0:ldf[len(ldf)-1].shape[1]-1].max())+1       \n",
    "        with data_output:\n",
    "            display(\"dim adr =\", dim_adr) #max(ldf[len(ldf)-1][0:len(ldf)-2].max())+1) \n",
    "        dim_grp = ldf[len(ldf)-1][\"IR\"].max()+1\n",
    "        with data_output:\n",
    "            display(\"dim grp = \", dim_grp)\n",
    "        #with data_output:\n",
    "            #display('---------------')\n",
    "        if dim_grp > dim_adr:\n",
    "            with data_output:\n",
    "                display(\"IR out of scale\")\n",
    "                dflosldf.iat[i,2] = \"oos\"\n",
    "                #display('------------------------------------------------')\n",
    "    return data_ini\n",
    "\n",
    "upload_file = widgets.FileUpload(multiple=False)\n",
    "upload_btn = widgets.Button(description=\"Upload\", icon='check', button_style='success')\n",
    "upload_btn.on_click(upload_btn_eventhandler)\n",
    "#data_ini = upload_btn.observe()\n",
    "\n",
    "#out = widgets.interactive_output(upload_btn_eventhandler, {})\n",
    "#display(widgets.HBox([out]))\n",
    "\n",
    "compute_btn = widgets.Button(description=\"Get Courno map\", icon='check', button_style='success')\n",
    "compute_btn.on_click(compute_btn_eventhandler)\n",
    "\n",
    "input_widgets = widgets.HBox([upload_file, upload_btn])\n",
    "columnsselected = widgets.Text(\n",
    "    value='ab',\n",
    "    placeholder='Type something',\n",
    "    description='column split:',\n",
    "    disabled=False\n",
    ")\n",
    "with split_output:\n",
    "    display(widgets.HBox([columnsselected]))\n",
    "with compute_btn_output:\n",
    "    display(compute_btn)\n",
    "with upload_btn_output:\n",
    "    display(input_widgets)\n",
    "\n",
    "# download\n",
    "# from ipywidgets import HTML\n",
    "# from IPython.display import display\n",
    "\n",
    "# import base64\n",
    "\n",
    "# #res = 'computed results'\n",
    "# res = data_ini\n",
    "# #FILE\n",
    "# filename = 'res.csv'\n",
    "# b64 = base64.b64encode(res.encode())\n",
    "# payload = b64.decode()\n",
    "\n",
    "# #BUTTONS\n",
    "# html_buttons = '''<html>\n",
    "# <head>\n",
    "# <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "# </head>\n",
    "# <body>\n",
    "# <a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" download>\n",
    "# <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download File</button>\n",
    "# </a>\n",
    "# </body>\n",
    "# </html>\n",
    "# '''\n",
    "\n",
    "# html_button = html_buttons.format(payload=payload,filename=filename)\n",
    "# display(HTML(html_button))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
